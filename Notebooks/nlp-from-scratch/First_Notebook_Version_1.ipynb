{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# What is Natural Language Processing?\n",
        "\n",
        "Natural Language Processing is processing of textual data/information. In Computer Science, NLP basically is a branch of AI which gives computers the ability to text and spoken words like humans. NLP enables computers to process voice and texts using Statistical/Probabilistic and Machine Learning and Deep Learning models to find **context** and select the most probable solution.\n",
        "\n",
        "## Natural Language Processing helps us in ways like:  \n",
        "*  Machine Translation:  \n",
        "\n",
        "   Machine translation goes beyond simple word-to-word translation to communicate the full meaning of the original language text in the target language. It analyzes all text elements and recognizes how the words influence one another. Literal translation might cause wrong translation of the words. We know how elements,structures like grammars and semantics play a vast role in providing contextual information in a sentence. Machine Translation effortlessly makes it less. We can see an interesting insight into that from the article below where machine translation has more to achieve as it is still not 100% accurate. So yeah, we as independent learners and researchers need to focus on this more because language is huge barrier in expressing what we think and feel. Removing that barrier as much as possible will help us move towards unimaginable possibilities in the future. The following link is an interesting read though. \n",
        "[The Most Common Translation Mistakes: How to Avoid Translation Mistakes in 2022](https://www.upwork.com/resources/translation-mistakes)  \n",
        "_A funny example_  \n",
        "![](https://github.com/SyedT1/NLP/blob/main/Notebooks/nlp-from-scratch/img/nb1/1.jpg?raw=true)  \n",
        "\n",
        " You can see more examples here on this [**link**](https://www.babbel.com/en/magazine/15-best-google-translate-fails)\n",
        "*   Chatbots and Virtual Assistants:\n",
        "\n",
        "    Virtual Assistants like Alexa or Siri takes human utterences whereas Chatbots taken written text as input and derives command to work upon that moving through decision trees.   \n",
        "\n",
        "*   Sentiment Analysis  \n",
        "    Takes emails and product review to derive sentiments from it. Like if a product review is positive or negative which is important for sales analysis for better feedback.\n",
        "\n",
        "*   Spam Detection  \n",
        "    Taking email message to check if a message is spam (urgency, inaccurate grammar, excessive/unnecessary usage of words).\n",
        "\n",
        "In whichever way an input is taken, doesn't matter, the words are tokenized and processed for lemmatization and stemming. Later on it is processed for parts of speech. Finally, we assign Named Entity Recognition to the words. For example take this sentence as an example: \n",
        "\n",
        "**\"Truth can never be hidden and it is a universal truth\"**\n",
        "\n",
        "<p style=\"color:#0000FF;\">After tokenization, this sentence gets split into words. Then, we try to find the common root of the words. Here, the root word of \"hidden\" is hide. This process is called stemming. Stemming is the process of removing suffix/prefix or tense from the word to get the root word. But, this doesn't always work. Here, universal has the root word \"universe\". In case of the word, university it is quite different. Then again, the word better can't have the root word \"bet\". Here, lemmatization comes into use. Contextual representations helps us distinguish in which way we should process the tokens.</p> Later on, we give tags to each word based on the way/placement in the sentence. This gives us context. Lastly, we assign NER(Named Entity Recognition) to the word."
      ],
      "metadata": {
        "id": "9wteZd9CuIDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Text Classification?\n",
        "Text classification is classification of unstructured text data into various categories. It is more like understanding the contents or the sentiment of text. For example: we have been able to detect political, religious, geo-political etc hate speech through text classification from various datasets i.e _reddit_, _twitter_ in the past.  \n",
        "![](https://github.com/SyedT1/NLP/blob/main/Notebooks/nlp-from-scratch/img/nb1/2.jpg?raw=true)  \n",
        "**_Using the model we do classification on the input data._**\n",
        "\n",
        "Text classification on reviews from the product can in turn improve user experience. "
      ],
      "metadata": {
        "id": "0g4Vfxd7VBLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why is Natural Language Processing different?\n",
        "\n",
        "Natural Language processing is different from other machine learning methods.\n",
        "Because, suppose that we're comparing Computer Vision with Natural Language Processing.    \n",
        "\n",
        "  Fundamentally, in layman's terms, using computer vision we are giving computers the ability to process an image, frame or series of frames for \n",
        "identifying image components. For example - if we are walking on the road, we need to look around us and make sure that there we are not getting hit by a barrier infront of us. We also have to look out for vehicles so that we don't get hit. Then, we are selecting something just by looking at the items, like , when we go to the shop to buy products. We don't just look at the items. We decide things quickly and accurately enough to make the right choice. So, in order for the computers to do the task for us, computer vision helps us. \n",
        "\n",
        " Whereas, in terms of Natural Language processing we're basically processing textual data in order to derive context or some meaning from it. For example-\n",
        " **\"Shit! Why did you do that man\"?**  \n",
        " This text contains curse words. And it shows anger. But, we want a computer to understand that. So that, it can do things for us. For example - when you are have an online business, there are plenty of customers you have to reach out to. Getting their reviews and then setting up parameters for better profit is time consuming. It will be easy if we train a machine learning model to analyse reviews for us quickly and effectively. \n"
      ],
      "metadata": {
        "id": "_-7FU3-0rmdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Bag of Words?\n",
        "\n",
        "Bag of words basically contains all the known words from a given corpus through which we can also find the presentce of those words in a document. At first sight, one might wonder why aren't the words ordered. Like lemme give an example:\n",
        "\n",
        "**I am a guy who loves to eat candies**  \n",
        "\n",
        "**Candies are bad for teeth**\n"
      ],
      "metadata": {
        "id": "CF2yJAR4xZ9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We take these two strings below and split them into words which were part of a sentence.**  \n",
        "\n",
        "For example:\n",
        "________________\n",
        "The string 'I am a guy who loves to eat candies' will be split into \n",
        "' I ' , ' am ' , ' a ' ,' guy ', ' who ', ' loves ', ' to ', ' eat ', ' candies '."
      ],
      "metadata": {
        "id": "o7t9sTd2YCRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str_arr1 = str('I am a guy who is bad loves to eat candies').split(' ')\n",
        "str_arr2 = str('Candies are bad for teeth').split(' ')\n",
        "dfinWords = []"
      ],
      "metadata": {
        "id": "dQkG1GpzXW_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, by now we should have removed **unnecesary punctuations, stopwords, special characters, articles**. But, for the sake of simplicity we will ignore it for now. We will pass all of these words into this common dictionary from both of these strings. Before that, we must make sure that they are all lower characters. We do this to ensure proper lemmatization and stemming. We are going to ignore that too for the simplicity of this tutorial _as of now_.  \n",
        "\n",
        "dfinWords is the common dictionary where the order of words shouldn't matter. And we will store all the known words."
      ],
      "metadata": {
        "id": "Bz4RhjOKXZpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in str_arr1:\n",
        "  if i.lower() not in dfinWords:\n",
        "    dfinWords.append(i.lower())\n",
        "for i in str_arr2:\n",
        "  if i.lower() not in dfinWords:\n",
        "    dfinWords.append(i.lower())\n",
        "print(dfinWords)\n",
        "print('size of the vocabulary is '+str(len(dfinWords)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IntLFXPXZzl",
        "outputId": "a33f69ec-c6c8-4752-c7a5-07e52b303b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'a', 'guy', 'who', 'is', 'bad', 'loves', 'to', 'eat', 'candies', 'are', 'for', 'teeth']\n",
            "size of the vocabulary is 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the code below, we are going to find which words occured in the dictionary. It will render us a binary value to give us an idea of whether the word occured or not.  \n",
        "For example:   \n",
        "'I am a guy who loves to eat candies ' will render us an array of values 0 or 1 to indicate whether that word **occured or not** in the dictionary."
      ],
      "metadata": {
        "id": "IHotEvtPYE8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetchit(l1,str):\n",
        "  list1 = []\n",
        "  for i in l1:\n",
        "    list1.append(str.count(i))\n",
        "  return list1\n",
        "calculate_bow1 = fetchit(dfinWords,str_arr1)\n",
        "print(dfinWords)\n",
        "print(calculate_bow1)\n",
        "calculate_bow2 = fetchit(dfinWords,str_arr2)\n",
        "print(calculate_bow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ4Mc2ZnYFFZ",
        "outputId": "71f6c62c-3d75-4816-9833-fadd7ff35398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'a', 'guy', 'who', 'is', 'bad', 'loves', 'to', 'eat', 'candies', 'are', 'for', 'teeth']\n",
            "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drawbacks of Bag-of-Words\n",
        "\n",
        "BOW offers flexibility for customization on specific data.\n",
        "\n",
        "It has been used on prediction problems like **_language modeling and documentation classification_**.  \n",
        "It's drawbacks are:   \n",
        " + **Vocabulary**: The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\n",
        " + **Sparsity**: Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space.\n",
        " + **Meaning**: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (“this is interesting” vs “is this interesting”), synonyms (“old bike” vs “used bike”), and much more.\n"
      ],
      "metadata": {
        "id": "I8g8mj6AbVPl"
      }
    }
  ]
}