{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wteZd9CuIDq"
      },
      "source": [
        "<h1 style=\"color:#FF0000;\">What is Natural Language Processing</h1>\n",
        "\n",
        "Natural Language Processing is processing of textual data/information. In Computer Science, NLP basically is a branch of AI which gives computers the ability to text and spoken words like humans. NLP enables computers to process voice and texts using Statistical/Probabilistic and Machine Learning and Deep Learning models to find **context** and select the most probable solution.\n",
        "\n",
        "<h1 style=\"color:#FF0000;\"> Natural Language Processing helps us in ways like</h1>  \n",
        "*  Machine Translation:  \n",
        "\n",
        "   Machine translation goes beyond simple word-to-word translation to communicate the full meaning of the original language text in the target language. It analyzes all text elements and recognizes how the words influence one another. Literal translation might cause wrong translation of the words. We know how elements,structures like grammars and semantics play a vast role in providing contextual information in a sentence. Machine Translation effortlessly makes it less. We can see an interesting insight into that from the article below where machine translation has more to achieve as it is still not 100% accurate. So yeah, we as independent learners and researchers need to focus on this more because language is huge barrier in expressing what we think and feel. Removing that barrier as much as possible will help us move towards unimaginable possibilities in the future. The following link is an interesting read though. \n",
        "[The Most Common Translation Mistakes: How to Avoid Translation Mistakes in 2022](https://www.upwork.com/resources/translation-mistakes)  \n",
        "_A funny example_  \n",
        "![](https://github.com/SyedT1/NLP/blob/main/Notebooks/nlp-from-scratch/img/nb1/1.jpg?raw=true)  \n",
        "\n",
        " You can see more examples here on this [**link**](https://www.babbel.com/en/magazine/15-best-google-translate-fails)\n",
        "*   Chatbots and Virtual Assistants:\n",
        "\n",
        "    Virtual Assistants like Alexa or Siri takes human utterences whereas Chatbots taken written text as input and derives command to work upon that moving through decision trees.   \n",
        "\n",
        "*   Sentiment Analysis  \n",
        "    Takes emails and product review to derive sentiments from it. Like if a product review is positive or negative which is important for sales analysis for better feedback.\n",
        "\n",
        "*   Spam Detection  \n",
        "    Taking email message to check if a message is spam (urgency, inaccurate grammar, excessive/unnecessary usage of words).\n",
        "\n",
        "In whichever way an input is taken, doesn't matter, the words are tokenized and processed for lemmatization and stemming. Later on it is processed for parts of speech. Finally, we assign Named Entity Recognition to the words. For example take this sentence as an example: \n",
        "\n",
        "**\"Truth can never be hidden and it is a universal truth\"**\n",
        "\n",
        "<p style=\"color:#0000FF;\">After tokenization, this sentence gets split into words. Then, we try to find the common root of the words. Here, the root word of \"hidden\" is hide. This process is called stemming. Stemming is the process of removing suffix/prefix or tense from the word to get the root word. But, this doesn't always work. Here, universal has the root word \"universe\". In case of the word, university it is quite different. Then again, the word better can't have the root word \"bet\". Here, lemmatization comes into use. Contextual representations helps us distinguish in which way we should process the tokens.</p> Later on, we give tags to each word based on the way/placement in the sentence. This gives us context. Lastly, we assign NER(Named Entity Recognition) to the word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g4Vfxd7VBLd"
      },
      "source": [
        "<h1 style=\"color:#FF0000\">What is Text Classification?</h1>\n",
        "Text classification is classification of unstructured text data into various categories. It is more like understanding the contents or the sentiment of text. For example: we have been able to detect political, religious, geo-political etc hate speech through text classification from various datasets i.e _reddit_, _twitter_ in the past.  \n",
        "![](https://github.com/SyedT1/NLP/blob/main/Notebooks/nlp-from-scratch/img/nb1/2.jpg?raw=true)  \n",
        "**_Using the model we do classification on the input data._**\n",
        "\n",
        "Text classification on reviews from the product can in turn improve user experience. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-7FU3-0rmdG"
      },
      "source": [
        "<h1 style=\"color:#FF0000;\">Why is Natural Language Processing different?</h1>\n",
        "\n",
        "Natural Language processing is different from other machine learning methods.\n",
        "Because, suppose that we're comparing Computer Vision with Natural Language Processing.    \n",
        "\n",
        "  Fundamentally, in layman's terms, using computer vision we are giving computers the ability to process an image, frame or series of frames for \n",
        "identifying image components. For example - if we are walking on the road, we need to look around us and make sure that there we are not getting hit by a barrier infront of us. We also have to look out for vehicles so that we don't get hit. Then, we are selecting something just by looking at the items, like , when we go to the shop to buy products. We don't just look at the items. We decide things quickly and accurately enough to make the right choice. So, in order for the computers to do the task for us, computer vision helps us. \n",
        "\n",
        " Whereas, in terms of Natural Language processing we're basically processing textual data in order to derive context or some meaning from it. For example-\n",
        " **\"Shit! Why did you do that man\"?**  \n",
        " This text contains curse words. And it shows anger. But, we want a computer to understand that. So that, it can do things for us. For example - when you are have an online business, there are plenty of customers you have to reach out to. Getting their reviews and then setting up parameters for better profit is time consuming. It will be easy if we train a machine learning model to analyse reviews for us quickly and effectively. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF2yJAR4xZ9Y"
      },
      "source": [
        "<h1 style=\"color:#FF0000;\"> What is Bag of Words?</h1>\n",
        "\n",
        "Bag of words basically contains all the known words from a given corpus through which we can also find the presentce of those words in a document. At first sight, one might wonder why aren't the words ordered. Like lemme give an example:\n",
        "\n",
        "**I am a guy who loves to eat candies**  \n",
        "\n",
        "**Candies are bad for teeth**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7t9sTd2YCRb"
      },
      "source": [
        "**We take these two strings below and split them into words which were part of a sentence.**  \n",
        "\n",
        "For example:\n",
        "________________\n",
        "The string 'I am a guy who loves to eat candies' will be split into \n",
        "' I ' , ' am ' , ' a ' ,' guy ', ' who ', ' loves ', ' to ', ' eat ', ' candies '."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQkG1GpzXW_K"
      },
      "outputs": [],
      "source": [
        "str_arr1 = str('I am a guy who is bad loves to eat candies').split(' ')\n",
        "str_arr2 = str('Candies are bad for teeth').split(' ')\n",
        "dfinWords = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz4RhjOKXZpv"
      },
      "source": [
        "Now, by now we should have removed **unnecesary punctuations, stopwords, special characters, articles**. But, for the sake of simplicity we will ignore it for now. We will pass all of these words into this common dictionary from both of these strings. Before that, we must make sure that they are all lower characters. We do this to ensure proper lemmatization and stemming. We are going to ignore that too for the simplicity of this tutorial _as of now_.  \n",
        "\n",
        "dfinWords is the common dictionary where the order of words shouldn't matter. And we will store all the known words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IntLFXPXZzl",
        "outputId": "a33f69ec-c6c8-4752-c7a5-07e52b303b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'am', 'a', 'guy', 'who', 'is', 'bad', 'loves', 'to', 'eat', 'candies', 'are', 'for', 'teeth']\n",
            "size of the vocabulary is 14\n"
          ]
        }
      ],
      "source": [
        "for i in str_arr1:\n",
        "  if i.lower() not in dfinWords:\n",
        "    dfinWords.append(i.lower())\n",
        "for i in str_arr2:\n",
        "  if i.lower() not in dfinWords:\n",
        "    dfinWords.append(i.lower())\n",
        "print(dfinWords)\n",
        "print('size of the vocabulary is '+str(len(dfinWords)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHotEvtPYE8n"
      },
      "source": [
        "From the code below, we are going to find which words occured in the dictionary. It will render us a binary value to give us an idea of whether the word occured or not.  \n",
        "For example:   \n",
        "'I am a guy who loves to eat candies ' will render us an array of values 0 or 1 to indicate whether that word **occured or not** in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ4Mc2ZnYFFZ",
        "outputId": "71f6c62c-3d75-4816-9833-fadd7ff35398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'am', 'a', 'guy', 'who', 'is', 'bad', 'loves', 'to', 'eat', 'candies', 'are', 'for', 'teeth']\n",
            "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "def fetchit(l1,str):\n",
        "  list1 = []\n",
        "  for i in l1:\n",
        "    list1.append(str.count(i))\n",
        "  return list1\n",
        "calculate_bow1 = fetchit(dfinWords,str_arr1)\n",
        "print(dfinWords)\n",
        "print(calculate_bow1)\n",
        "calculate_bow2 = fetchit(dfinWords,str_arr2)\n",
        "print(calculate_bow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8g8mj6AbVPl"
      },
      "source": [
        "<h1 style=\"color:#FF0000;\">Drawbacks of Bag-of-Words</h1>\n",
        "\n",
        "BOW offers flexibility for customization on specific data.\n",
        "\n",
        "It has been used on prediction problems like **_language modeling and documentation classification_**.  \n",
        "It's drawbacks are:   \n",
        " + **Vocabulary**: The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\n",
        " + **Sparsity**: Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space.\n",
        " + **Meaning**: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (“this is interesting” vs “is this interesting”), synonyms (“old bike” vs “used bike”), and much more.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1 style=\"color:#FF0000;\">Support Vector Machines</h1>\n",
        "<p>\n",
        "Suppose, you went to a grocery store to purchase fruits. You wanted to buy an apple and an orange. There you saw an apple that looks like an orange. It will be challenging for you. It will be quite difficult for a machine to identify it . Here, comes SVM to the rescue.\n",
        "</p>\n",
        "<p>\n",
        " SVM looks at the extremes of the dataset and draws a decision boundary known as the hyperplane near the extreme point in the dataset.\n",
        "</p>\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/SyedT1/NLP/blob/main/img/SVM/svm3.png?raw=true\" alter=\"img2\">\n",
        "<figcaption><i>We can draw a decision boundary in the following manner classifying based on the ear geometry and snout length features</i></figcaption>\n",
        "</figure>\n",
        "<p> If we have unoptimized decision boundary, that might result in a lot of misclassifications based on new data. </p>\n",
        "\n",
        "<p> <b>Support Vectors</b> are defined as data points that the margins pushes up against or the points close to the opposing classes.\n",
        "If we see a dog that looks like a cat or a cat that looks a dog, we want a decision boundary to classify the new data properly.\n",
        "</p>\n",
        "\n",
        "<p> We have <b> D+ </b> which is the shortest distance to the closest possible point. And we have <b> D- </b> which is the shortest distance to the closest negative point.</p>\n",
        "<figure>\n",
        "<img src=\"https://github.com/SyedT1/NLP/blob/main/img/SVM/Dpos_neg.png?raw=true\">\n",
        "<figcaption> <i>We have a margin of a separating hyperplane which is D+ along with D- </i><figcaption>\n",
        "</figure>\n",
        "<p> The line or decision boundary (line at the middle) that segragates the two classes is known as the hyperplane. Because SVM can be used in multidimensional datasets and the datapoints are knowns are <i><b>vectors</i></b></p>\n",
        "\n",
        "<p> We have discussed so far about <i><b>Linear Support Vector Machines</b></i> as the classes are linearly separable. </p>\n",
        "<p> Now there are cases where the datapoints are not <i>linearly separable</i>.</p>\n",
        "\n",
        "<h3> Here is it almost Impossible to separate the classes using a single line in the following image</h3>\n",
        "<img src=\"https://github.com/SyedT1/NLP/blob/main/img/SVM/Screenshot%20from%202022-12-09%2018-41-22.png?raw=true\">\n",
        "<p> Here we can use a function to convert the data into high dimensional space where we can easily separate the two classes. </p>\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://github.com/SyedT1/NLP/blob/main/img/SVM/Screenshot%20from%202022-12-09%2018-44-51.png?raw=true\">\n",
        "  <figcaption>Converting from 1D to 2D (a parabola) by applying a simple polynomial function , helped us to draw a hyperplane</figcaption>\n",
        "</figure>\n",
        "\n",
        "<p> We can do the same for other examples where we are trying to separate the two classes in a 2 dimensional plane, by applying some function we can transform it into a 3D space. But, the problem with conversion of data into higher dimensional feature space is, that, it is computatonal expensive. We can use a <b><i>Kernel Trick</i></b> to reduce the computational cost.</p> \n",
        "<p> The function that takes input vectors in the original space and returns the dot product of the vectors in the feature space in known as <i><b>kernel function</b></i> or <i><b>kernel trick.</i></b>. \n",
        "We can apply the dot product between two vectors so that every point is mapped into a high dimensional space via transformation.</p> \n",
        "<p> We use it to transform a non-linear space into a linear space. </p>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mathematical Derivation:\n",
        "<p>SVM uses the kernel trick to model non-linear decision boundaries</p>\n",
        "<p> Each point is represented by some feature vector <b>x</b> such that</p>\n",
        "\n",
        "$$ x \\in \\mathbb{R}$$\n",
        "\n",
        "Now, if it's difficult for us to classify based on this, we want to map this to a more non-linear feature space such as \n",
        "\n",
        "$$\n",
        "\\phi: \\mathbb{R}^D \\rightarrow \\mathbb{R}^M\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\phi(x) \\in \\mathbb{R}^M\n",
        "$$\n",
        "\n",
        "Thus, we have transformed the feature space where each of these input features x mapped to a transformed basis vector Phi(x)\n",
        "\n",
        "\n",
        "<p><b><i>Hyperplanes</i></b> mentioned above in the discussion is represented as:</p>\n",
        "\n",
        "\n",
        "$$\n",
        "H: w^{\\top} \\phi(x)+b=0\n",
        "$$\n",
        "\n",
        "Here, b is a <b>bias</b> term. A hyperplane is D-1 separator\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advantages of SVM:\n",
        "+ Effective in High Dimensional Space\n",
        "+ Number of dimensions are greater than number of samples\n",
        "+ Various kernel functions can be specified for various decision functions.\n",
        "+ We can add more kernel functions to get more complex hyperplanes.\n",
        "\n",
        "# Disadvantages of SVM\n",
        "+ Number of features > Number of samples gives poor performance\n",
        "+ Do not provide probability estimates"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
